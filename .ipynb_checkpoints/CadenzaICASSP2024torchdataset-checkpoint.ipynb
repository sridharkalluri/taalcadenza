{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4fc7a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Load dependencies, define functions, and formulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966213f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from clarity.utils.file_io import read_signal\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import re\n",
    "import librosa\n",
    "import python_auditory_toolbox.auditory_toolbox as pat\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy import ndarray\n",
    "from pathlib import Path\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "\n",
    "def load_hrtf_signals(hrtf_path: str, hp: dict) -> tuple[ndarray, ndarray]:\n",
    "    \"\"\"Loads the HRTF signals for a given head position.\n",
    "\n",
    "    Args:\n",
    "        hrtf_path (str): Path to the HRTF signals.\n",
    "        hp (dict): Head position.\n",
    "\n",
    "    Returns:\n",
    "        tuple(ndarray, ndarray): Left and right HRTF signals.\n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    Requires Path, json, numpy, scipy, clarity.utils.file_io\n",
    "    '''\n",
    "    \n",
    "    hp_left_path = (\n",
    "        Path(hrtf_path) / f\"{hp['mic']}-{hp['subject']}-n{abs(hp['left_angle'])}.wav\"\n",
    "    )\n",
    "    hp_right_path = (\n",
    "        Path(hrtf_path) / f\"{hp['mic']}-{hp['subject']}-p{abs(hp['right_angle'])}.wav\"\n",
    "    )\n",
    "\n",
    "    hp_left_signal = read_signal(hp_left_path)\n",
    "    hp_right_signal = read_signal(hp_right_path)\n",
    "\n",
    "    return hp_left_signal, hp_right_signal\n",
    "\n",
    "def mixmatrix_to_demix(lefthrir: ndarray, righthrir: ndarray) -> tuple[ndarray, ndarray, ndarray, ndarray]:\n",
    "    \"\"\"\n",
    "    Compute demix impulse responses from mix impulse responses\n",
    "\n",
    "    Args:\n",
    "        lefthrir: binaural impulse response (left ear in col 0, right ear in col 1) for signal from left loudspeaker\n",
    "        righthrir: binaural impulse response (left ear in col 0, right ear in col 1) for signal from right loudspeaker\n",
    "\n",
    "    Returns:\n",
    "        tuple(ndarray, ndarray): demix impulse response to reconstruct left loudspeaker and right loudspeaker\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    Requires numpy\n",
    "    '''\n",
    "    \n",
    "    ## do demix in the frequency domain, and then invert to get back to time-domain impulse responses\n",
    "    irlen = len(lefthrir)\n",
    "    \n",
    "    #compute freq domain representations of head related impulse responses\n",
    "    NFFT = 2**np.ceil(np.log2(irlen))\n",
    "    leftHRTF = fft(lefthrir,256,axis=0)\n",
    "    rightHRTF = fft(righthrir,256,axis=0)\n",
    "    \n",
    "    #assemble mixing matrix, (experiment later with assembling only 1/2 the matrix and\n",
    "    #assembling the other half by complex conjugation)\n",
    "    #H = [leftHRTF[:,0] rightHRTF[:,0]\n",
    "    #     leftHRTF[:,1] rightHRTF[:,0]]\n",
    "    Hw = np.concatenate((leftHRTF[:,:,np.newaxis],rightHRTF[:,:,np.newaxis]),axis=2)\n",
    "\n",
    "    #de-mix matrix at every frequency is the matrix inverse of the mix matrix \n",
    "    #numpy.linalg.inv separately inverts each row of the NFFTx2x2 nd-array and returns the result\n",
    "    #in a NFFTx2x2 nd-array\n",
    "    # ***** N.B. the inverted transfer function / impulse response does not behave well at high frequencies\n",
    "    # and low frequencies, so may need to smooth the spectrum before inverting *****\n",
    "    \n",
    "    Gw = np.linalg.inv(Hw)\n",
    "    demixir = ifft(Gw,axis=0)\n",
    "    lefteardemix = demixir[:irlen,0,:]\n",
    "    righteardemix = demixir[:irlen,1,:]\n",
    "\n",
    "    return lefteardemix, righteardemix\n",
    "\n",
    "def bernsteinenvelopecompression(multichansig, sample_rate):\n",
    "    '''\n",
    "    halfwave rectify then full envelope compression ...\n",
    "   \n",
    "    The envelope compression itself is from Bernsten, van de Par\n",
    "    and Trahiotis (1996, especially the Appendix). The\n",
    "    lowpass filtering is from Berstein and Trahiotis (1996,\n",
    "    especially eq 2 on page 3781). \n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Requires numpy, scipy\n",
    "    '''\n",
    "\n",
    "\n",
    "    #envelope compression using Weiss/Rose lowpass filter\n",
    "    compress1 = 0.23;\n",
    "    compress2 = 2.0;\n",
    " \n",
    "    # define lowpass filter\n",
    "    cutoff = 425; #Hz\n",
    "    order = 4;\n",
    "\n",
    "    lpf = np.linspace(0, sample_rate/2, 10000)\n",
    "    f0 = cutoff * (1/ (2**(1/order)-1)**0.5)\n",
    "    lpmag = 1 / (1+(lpf/f0)**2) ** (order/2)\n",
    "    lpf=lpf / (sample_rate/2);\n",
    "    f=lpf\n",
    "    m=lpmag\n",
    "    m[-1]=0\n",
    "    lowpassfiltercoeffs = scipy.signal.firwin2(256, f, m, window='hamming')\n",
    "    \n",
    "    # compress each filter! \n",
    "    envelope = np.abs(scipy.signal.hilbert(multichansig)) #hilbert envelope\n",
    "    compressedenvelope = (envelope**(compress1-1))*multichansig #power-law compression\n",
    "    rectifiedenvelope = np.maximum(compressedenvelope, 0)**compress2 #half-wave rectification and raise to power2\n",
    "    multichanout = scipy.signal.oaconvolve(rectifiedenvelope,lowpassfiltercoeffs[np.newaxis,:],mode='same',axes=1)\n",
    "    \n",
    "    '''\n",
    "    %Akeroyd's MATLAB code\n",
    "    nfilters= size(multichansig,1)\n",
    "    for filter in 1:nfilters\n",
    "    % get envelope\n",
    "        envelope = abs(hilbert(multichanneldata(filter,:)));\n",
    "        % compress the envelope to a power of compression1, while maintaining\n",
    "        % the fine structure. \n",
    "        compressedenvelope = (envelope.^(compress1 - 1)).*multichanneldata(filter,:);\n",
    "        % rectify that compressed envelope \n",
    "        rectifiedenvelope = compressedenvelope;\n",
    "        findoutput = find(compressedenvelope<0);\n",
    "        rectifiedenvelope(findoutput) = zeros(size(findoutput));\n",
    "        % raise to power of compress2\n",
    "        rectifiedenvelope = rectifiedenvelope.^compress2;\n",
    "        % overlap-add FIR filter using the fft\n",
    "        multichanneldata2(filter,:) = fftfilt(lowpassfiltercoefficients, rectifiedenvelope);\n",
    "    end\n",
    "    '''    \n",
    "    return multichanout\n",
    "\n",
    "def pairwisecov(x,y):\n",
    "    '''\n",
    "    pairwisecov(x,y) computes covariance matrices of corresponding rows in 2D arrays x and y\n",
    "    x, y have same dimensions, with different variables on each row and repeated observations on each column and can include NaN entries\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Requires numpy, pandas\n",
    "    '''\n",
    "    \n",
    "    # Initialize an empty list to store the covariance matrices\n",
    "    cov_matrices = []\n",
    "\n",
    "    # Compute pairwise covariance matrices\n",
    "    for i in range(x.shape[0]):\n",
    "        # Convert the i-th rows of x and y into pandas Series\n",
    "        s1 = pd.Series(x[i])\n",
    "        s2 = pd.Series(y[i])\n",
    "    \n",
    "        # Compute the covariance matrix and handle NaN entries\n",
    "        cov_matrix = np.array([[s1.cov(s1, min_periods=1), s1.cov(s2, min_periods=1)], \n",
    "                               [s2.cov(s1, min_periods=1), s2.cov(s2, min_periods=1)]])\n",
    "    \n",
    "        cov_matrices.append(cov_matrix)\n",
    "\n",
    "    # Convert the list of covariance matrices into a 3D numpy array\n",
    "    cov_matrices = np.array(cov_matrices)\n",
    "\n",
    "    return cov_matrices\n",
    "\n",
    "\n",
    "def binauralanalysis(scenefbankleft,scenefbankright):    \n",
    "    '''\n",
    "    Requires numpy, librosa, scipy, pandas\n",
    "    '''\n",
    "    \n",
    "    MAXITD = 1 #milliseconds\n",
    "    MAXILD = 10 #dB\n",
    "    COHTHRESH = 0.9 #threshold interaural coherence\n",
    "    SAMPLE_RATE = 44100 #Hz\n",
    "    \n",
    "    WINDOW = 20 #milliseconds, 10 ms a la Faller and Merimaa\n",
    "    minframelen = WINDOW/1000 * SAMPLE_RATE\n",
    "    framelen =  2**np.ceil(np.log2(minframelen))\n",
    "    hop_len = framelen/2 #the relatively large hop_len is a form of smoothing akin to Faller and Merimaa\n",
    "    \n",
    "    if 1:\n",
    "        compleft = bernsteinenvelopecompression(scenefbankleft, SAMPLE_RATE)\n",
    "        compright = bernsteinenvelopecompression(scenefbankright,SAMPLE_RATE)\n",
    "    \n",
    "        leftframes = librosa.util.frame(compleft,frame_length = framelen.astype(int), hop_length = hop_len.astype(int), axis = 1)\n",
    "        rightframes = librosa.util.frame(compright,frame_length = framelen.astype(int), hop_length = hop_len.astype(int), axis = 1)\n",
    "    else:\n",
    "        leftframes = librosa.util.frame(scenefbankleft,frame_length = framelen.astype(int), hop_length = hop_len.astype(int), axis = 1)\n",
    "        rightframes = np.copy(leftframes)\n",
    "    \n",
    "    nchans, nframes, _ = leftframes.shape\n",
    "    \n",
    "    '''\n",
    "    Compute ITD, ILD, ICC akin to Faller and Merimaa.\n",
    "    Current code does not smooth the ITD, ILD, and ICC estimates in the manner of Faller and Merimaa. They smoothed the estimates\n",
    "    using a first-order IIR filter. Might be something to incorporate down the line.\n",
    "    '''\n",
    " \n",
    "    #######\n",
    "    ildest = 10*np.log10(np.mean(leftframes**2,axis = -1) / np.mean(rightframes**2,axis=-1))\n",
    "    #prefer scipy.signal.fftconvolve over scipy.signal.correlate due to better control over matrix axes over which computation is performed\n",
    "    binxcorr = scipy.signal.fftconvolve(leftframes,np.flip(rightframes,axis=-1),mode='full',axes=-1) #compute cross-correlation in FFT domain, by filtering left frame with reversed right frame\n",
    "    #leftxcorr = scipy.signal.fftconvolve(leftframes,np.flip(leftframes,axis=-1),mode='full',axes=-1) #autocorrelation\n",
    "    #rightxcorr = scipy.signal.fftconvolve(rightframes,np.flip(rightframes,axis=-1),mode='full',axes=-1) #autocorrelation\n",
    "    leftxcorrlag0 = np.sum(leftframes**2,axis=-1,keepdims=True)\n",
    "    rightxcorrlag0 = np.sum(rightframes**2,axis=-1,keepdims=True)\n",
    "    cohlags = scipy.signal.correlation_lags(framelen,framelen) #correlation lag in samples\n",
    "    iacoh = binxcorr / (np.tile(leftxcorrlag0,len(cohlags))*np.tile(rightxcorrlag0,len(cohlags)) + np.finfo(np.float64).eps)**0.5 #normalized interaural coherence (need to check if normalization is done appropriately) \n",
    "      \n",
    "    lagsms = cohlags*1/SAMPLE_RATE*1000 #correlation lag in milliseconds\n",
    "    validlags = np.logical_and(lagsms >= -MAXITD, lagsms <= MAXITD)\n",
    "    iamaxcohest = np.max(iacoh[:,:,validlags],axis=-1) #in each frame, IAC is the maximum coherence in the range of valid correlation lags\n",
    "    itdindices = np.min(np.argwhere(validlags)) + np.argmax(iacoh[:,:,validlags],axis=-1) #in each frame, ITD is the lag at which IAC is maximum in the range of valid correlation lags\n",
    "    itdest = lagsms[itdindices] #convert lags in units of samples into units of milliseconds\n",
    "\n",
    "    #suppress ITD, ILD estimates in frames where normalized interaural coherence < COHTHRESH\n",
    "    cohthreshind = iamaxcohest < 0.9\n",
    "    #invalind = np.logical_or(itdest < -MAXITD, itdest > MAXITD)\n",
    "    itd = np.copy(itdest)\n",
    "    ild = np.copy(ildest)\n",
    "    #itd[np.logical_or(invalind,cohthreshind)] = np.NaN\n",
    "    itd[cohthreshind] = np.NaN\n",
    "    ild[cohthreshind] = np.NaN\n",
    "\n",
    "    #compute descriptive frequency statistics of ITD, ILD, and ICC\n",
    "    prophighcoh = 1 - np.sum(cohthreshind,axis=-1) / nframes\n",
    "    meancoh = np.mean(iamaxcohest,axis=-1)\n",
    "    #itd stats\n",
    "    meanselitd = np.nanmean(itd,axis=-1)\n",
    "    #stdselitd = np.nanstd(itd,axis=-1) #already \n",
    "    medselitd = np.nanmedian(itd,axis=-1)\n",
    "    itdpercentiles = np.nanpercentile(itd,[10, 25, 75, 90],axis=-1)\n",
    "\n",
    "    #ild stats\n",
    "    meanselild = np.nanmean(ild,axis=-1)\n",
    "    #stdselitd = np.nanstd(itd,axis=-1)\n",
    "    medselild = np.nanmedian(ild,axis=-1)\n",
    "    ildpercentiles = np.nanpercentile(ild,[10, 25, 75, 90],axis=-1)\n",
    "\n",
    "    #joint itd, ild stats\n",
    "    covmats = pairwisecov(itd,ild)\n",
    "    unselcovmats = pairwisecov(itdest,ildest)\n",
    "\n",
    "    #aggregate stats into single feature vector\n",
    "    #meancoh, meanselitd, medselitd, itdpercentiles, meanselild, medselild, ildpercentiles, covmats, unselcovmats\n",
    "    binfeatvec = np.concatenate([prophighcoh, meancoh, meanselitd, medselitd, np.ndarray.flatten(itdpercentiles),\n",
    "                                 meanselild, medselild, np.ndarray.flatten(ildpercentiles), \n",
    "                                 np.ndarray.flatten(covmats), np.ndarray.flatten(unselcovmats)])\n",
    "       \n",
    "    return binfeatvec\n",
    "\n",
    "def featureanalysis(signal):   \n",
    "    '''\n",
    "    Requires numpy, librosa, scipy, pandas, python_auditory_toolbox, torch\n",
    "    '''\n",
    "    \n",
    "    N_FFT = 512\n",
    "    N_HOP = 16\n",
    "    SAMPLE_RATE = 44100\n",
    "    N_BINS = 32\n",
    "    NUM_CHAN = 16 #need to figure out number and spacing of channels such that each channel is 1-ERB wide\n",
    "    LOW_FREQ = 100\n",
    "    \n",
    "    #monaural features\n",
    "    stft = librosa.stft(y = signal.numpy(),n_fft=N_FFT,hop_length=N_HOP)\n",
    "    mstft, pstft = librosa.magphase(stft)\n",
    "    #compute spectral centroid, spectral contrast, spectral bandwidth\n",
    "    #librosa routines keep the two ears separated\n",
    "    centroid = librosa.feature.spectral_centroid(S = mstft, sr = SAMPLE_RATE, n_fft = N_FFT, hop_length = N_HOP)\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(S = mstft, sr = SAMPLE_RATE, n_fft = N_FFT,\n",
    "                                                    hop_length = N_HOP, centroid = centroid)\n",
    "    contrast = librosa.feature.spectral_contrast(S = mstft, sr = SAMPLE_RATE, n_fft = N_FFT,\n",
    "                                                    hop_length = N_HOP)\n",
    "    #probability density of spectral centroid, spectral contrast, spectral bandwidth\n",
    "    #(down the line, include other music features like rhythm features, etc)\n",
    "    #The two ears get pooled into a single histogram by numpy.histogram\n",
    "    def histproportion(x,nbins):\n",
    "        dens, be = np.histogram(x,bins=N_BINS,density=True)\n",
    "        return dens*np.diff(be), be\n",
    "\n",
    "    dcentroid, bec = histproportion(centroid,N_BINS)\n",
    "    dbandwidth, beb = histproportion(bandwidth,N_BINS)\n",
    "    dcontrast, beco = histproportion(contrast,N_BINS)\n",
    "    #featvec = np.torch([avgslpcnt, avgiacnt.unsqueeze(dim=0), avgslpiacnt.unsqueeze(dim=0)])    \n",
    "    monfeats = np.concatenate([dcentroid, bec, dbandwidth, beb, dcontrast, beco])\n",
    "    \n",
    "    #binaural features\n",
    "    fcoefs = pat.MakeErbFilters(SAMPLE_RATE, NUM_CHAN, LOW_FREQ)\n",
    "    scenefbankleft = pat.ErbFilterBank(signal.numpy()[0], fcoefs)\n",
    "    scenefbankright = pat.ErbFilterBank(signal.numpy()[1], fcoefs)    \n",
    "    binfeats = binauralanalysis(scenefbankleft,scenefbankright)\n",
    "    \n",
    "    featvec = torch.from_numpy(np.concatenate([monfeats, binfeats]))\n",
    "    \n",
    "    # ****need to check that the feature vector has good properties like 0-mean, unit-std *****\n",
    "    #featvec = np.torch([avgslpcnt, avgiacnt.unsqueeze(dim=0), avgslpiacnt.unsqueeze(dim=0)])\n",
    "    \n",
    "    return featvec\n",
    "\n",
    "def extract_text_after_pattern(input_string, pattern):    \n",
    "    '''\n",
    "    Requires Re package (regular expressions)\n",
    "    '''    \n",
    "    \n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        return input_string[match.end():]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "class FeaturesCalculator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.spec_centroid = torch.nn.Sequential(\n",
    "        #    #torchaudio.transforms.Resample(input_freq, resample_freq),\n",
    "        #    torchaudio.transforms.SpectralCentroid(sample_rate = resample_freq, n_fft= n_fft, hop_length = n_hop),\n",
    "        #)\n",
    "\n",
    "\n",
    "    def forward(self, signal: torch.Tensor) -> torch.Tensor:        \n",
    "        featvec = featureanalysis(signal)\n",
    "        return featvec\n",
    "\n",
    "class AtMicCadenzaICASSP2024(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 split = \"train\", #or \"valid\"\n",
    "                 audiodir = \"/Users/sridhar/Documents/Projects/clarity/clarity/cad_icassp_2024/audio/at_mic_music\",\n",
    "                 hrtfdir = \"/Users/sridhar/Documents/Projects/clarity/clarity/cad_icassp_2024/audio/hrtf\",\n",
    "                 metadatadir = \"/Users/sridhar/Documents/Projects/clarity/clarity/cad_icassp_2024/metadata\",\n",
    "                 transform=None,\n",
    "                 target_transform=None):\n",
    "\n",
    "        #Determine if training or validation split\n",
    "        self.split = split\n",
    "        \n",
    "        # Load the scenes metadata\n",
    "        scenes_metafile = Path(metadatadir,f\"at_mic_music.{split}.json\")\n",
    "        with open(scenes_metafile, encoding = \"utf-8\") as f:\n",
    "            scenes_metadata = json.load(f)\n",
    "        self.scenes_metadata = scenes_metadata\n",
    "        self.scenes_names = list(scenes_metadata.keys())\n",
    "        \n",
    "        # Load the spatial configurations metadata\n",
    "        spatconfigs_file = Path(metadatadir,\"head_loudspeaker_positions.json\")\n",
    "        with open(spatconfigs_file, encoding=\"utf-8\") as f:\n",
    "            spatconfigs_metadata = json.load(f)\n",
    "        self.spatconfigs_metadata = spatconfigs_metadata\n",
    "        self.spatconfigslabels = list(spatconfigs_metadata.keys())\n",
    "            \n",
    "        # Define location of audio files & hrtf files\n",
    "        self.audiodir = audiodir\n",
    "        self.hrtfdir = hrtfdir\n",
    "        \n",
    "        # Define object methods\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.calculator = FeaturesCalculator()\n",
    "        self.calculator.to(device)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenes_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                                 \n",
    "        #get single scene name \n",
    "        item_scenemetadata = self.scenes_metadata[f\"{self.scenes_names[idx]}\"]\n",
    "        item_scenepath = item_scenemetadata[\"Path\"]\n",
    "        item_audname = Path(self.audiodir,item_scenepath,'mixture.wav')\n",
    "        #read the binaural audio of the recorded scene from disk\n",
    "        audscene, sample_rate = torchaudio.load(item_audname)\n",
    "        features = self.calculator(audscene)\n",
    "                                 \n",
    "        #get dict describing spatial configuration of the scene        \n",
    "        if self.split == 'train':\n",
    "            idxitem_scenedict = self.spatconfigs_metadata[f\"{item_scenemetadata['Head Position']}\"]\n",
    "        elif self.split == 'valid':\n",
    "            extract = extract_text_after_pattern(item_scenemetadata['Track Name'],r\"hlp\")\n",
    "            item_headpos = f\"hlp{extract}\"\n",
    "            idxitem_scenedict = self.spatconfigs_metadata[item_headpos]\n",
    "        else:\n",
    "            idxitem_scenedict = []\n",
    "            \n",
    "        if not idxitem_scenedict:\n",
    "            target = torch.zeros(1)        \n",
    "        else:\n",
    "            #get hrtfs corresponding to the spatial configuration of the scene, where\n",
    "            #hL is the binaural pair of impulse responses for stimulus at Left loudspeaker\n",
    "            #hR is the binaural pair of impulse responses for stimulus at right loudspeaker\n",
    "            hL, hR = load_hrtf_signals(self.hrtfdir,idxitem_scenedict)\n",
    "            #gl is the pair of demix impulse responses to apply to left ear\n",
    "            #gr is the pair of demix impulse responses to apply to right ear\n",
    "            gl, gr = mixmatrix_to_demix(hL, hR)\n",
    "            target = np.concatenate((np.reshape(gl,(len(gl)*2, 1),'F'),np.reshape(gr,(len(gr)*2,1),'F')),axis=0)\n",
    "            target = torch.from_numpy(target)\n",
    "        \n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b604895",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Initialize file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d54db5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadatadir = \"/Users/sridhar/Documents/Projects/clarity/clarity/cad_icassp_2024/metadata\"\n",
    "hrtf_dir = \"/Users/sridhar/Documents/Projects/clarity/clarity/cad_icassp_2024/audio/hrtf\"\n",
    "audio_dir = \"/Users/sridhar/Documents/Projects/clarity/clarity/cad_icassp_2024/audio/at_mic_music\"\n",
    "split = \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca97077",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. (optional) Test dataset functionalities, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00d114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2.1 On reading metadata and audio files\n",
    "\n",
    "scenes_metafile = Path(metadatadir,f\"at_mic_music.{split}.json\")\n",
    "with open(scenes_metafile, encoding = \"utf-8\") as f:\n",
    "    scenes_metadata = json.load(f)\n",
    "    \n",
    "head_loudspeaker_positions_file = Path(metadatadir,\"head_loudspeaker_positions.json\")\n",
    "with open(head_loudspeaker_positions_file, encoding=\"utf-8\") as f:\n",
    "    head_positions_metadata = json.load(f)\n",
    "\n",
    "scenes_names = list(scenes_metadata.keys())\n",
    "idx = 146\n",
    "item_scenemetadata = scenes_metadata[f\"{scenes_names[idx]}\"]\n",
    "item_scenepath = item_scenemetadata[\"Path\"]\n",
    "item_audname = Path(audio_dir,item_scenepath,'mixture.wav')\n",
    "\n",
    "spatconfigslabels = list(head_positions_metadata.keys())\n",
    "idxitem_scenedict = head_positions_metadata[f\"{item_scenemetadata['Head Position']}\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571bd81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2.2 Examine HRTFs, and demix HRIR/HRTFs\n",
    "\n",
    "hL, hR = load_hrtf_signals(hrtf_dir,idxitem_scenedict)\n",
    "Hw, Gw, leardemix, reardemix = mixmatrix_to_demix(lhrtf,rhrtf)\n",
    "\n",
    "plt.plot(20*np.log10(np.abs(Hw[:256//2,:,0])))\n",
    "\n",
    "plt.plot(20*np.log10(np.abs(Gw[:256//2,:,1])))\n",
    "\n",
    "llhrir = lhrtf\n",
    "llhrir[:,1] = 1e-15\n",
    "rrhrir = rhrtf\n",
    "rrhrir[:,0] = 1e-15\n",
    "\n",
    "Hwnomix, Gwnomix, leardenomix, reardenomix = mixmatrix_to_demix(llhrir,rrhrir)\n",
    "\n",
    "plt.plot(20*np.log10(np.abs(Hwnomix[:256//2,1,1])))\n",
    "\n",
    "plt.plot(20*np.log10(np.abs(Gwnomix[:256//2,1,1])))\n",
    "\n",
    "plt.plot(rrhrir[:,1])\n",
    "\n",
    "plt.plot(reardenomix[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e9ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2.3 Read a sample audio file\n",
    "\n",
    "scenes_names = list(scenes_metadata.keys())\n",
    "#get single scene name \n",
    "item_scenemetadata = scenes_metadata[f\"{scenes_names[0]}\"]\n",
    "item_scenepath = item_scenemetadata[\"Path\"]\n",
    "item_audname = Path(audio_dir,item_scenepath,'mixture.wav')\n",
    "#read the binaural audio of the recorded scene from disk\n",
    "audscene, sample_rate = torchaudio.load(item_audname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770e449",
   "metadata": {},
   "source": [
    "## 3. Read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54128ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = AtMicCadenzaICASSP2024(\n",
    "    split=\"train\",\n",
    "    metadatadir = metadatadir,\n",
    "    hrtfdir = hrtf_dir,\n",
    "    audiodir = audio_dir,\n",
    ")\n",
    "\n",
    "test_data = AtMicCadenzaICASSP2024(\n",
    "    split=\"valid\",\n",
    "    metadatadir = metadatadir,\n",
    "    hrtfdir = hrtf_dir,\n",
    "    audiodir = audio_dir,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b2b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/jc2lb6zs5f5b9pb1w2ms4g5h0000gn/T/ipykernel_58393/159706856.py:124: RuntimeWarning: divide by zero encountered in power\n",
      "  compressedenvelope = (envelope**(compress1-1))*multichansig #power-law compression\n",
      "/Users/sridhar/anaconda3/envs/clarity/lib/python3.10/site-packages/scipy/signal/_signaltools.py:510: RuntimeWarning: invalid value encountered in multiply\n",
      "  ret = ifft(sp1 * sp2, fshape, axes=axes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Review features and target.\n",
    "train_features, train_target = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_target.size()}\")\n",
    "\n",
    "#test_features, test_target = next(iter(train_dataloader))\n",
    "#print(f\"Feature batch shape: {train_features.size()}\")\n",
    "#print(f\"Labels batch shape: {train_target.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f967276",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Test monaural transduction model (Bernstein and Trahiotis envelope compression)\n",
    "\n",
    "import scipy\n",
    "import python_auditory_toolbox.auditory_toolbox as pat\n",
    "NUM_CHAN = 16\n",
    "LOW_FREQ = 100\n",
    "SAMPLE_RATE = 44100\n",
    "fcoefs = pat.MakeErbFilters(SAMPLE_RATE, NUM_CHAN, LOW_FREQ)\n",
    "scenefbankleft = pat.ErbFilterBank(audscene.numpy()[0], fcoefs)\n",
    "scenefbankright = pat.ErbFilterBank(audscene.numpy()[1], fcoefs)\n",
    "compleft = bernsteinenvelopecompression(scenefbankleft, SAMPLE_RATE)\n",
    "compright = bernsteinenvelopecompression(scenefbankright, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d3415-4a13-4bd6-8071-d0dbdd509d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "calculator = FeaturesCalculator()\n",
    "calculator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ca9cf-642b-4f9e-8609-22a1d0a4c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = calculator(audscene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c56bd",
   "metadata": {},
   "source": [
    "## 4. Define Neural network model of the de-mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(547, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 235*4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        pred = self.linear_relu_stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bac22d-067e-4adb-9a6e-23f3acd0f979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
